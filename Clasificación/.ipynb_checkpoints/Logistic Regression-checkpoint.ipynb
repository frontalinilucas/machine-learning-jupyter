{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística\n",
    "\n",
    "- Modelo Supervizado de CLASIFICACIÓN.\n",
    "\n",
    "Un criterio para modelizar el dataset es plantear que de acuerdo a la posición de un patrón en particular en el plano, ese patrón tiene una probabildad P de pertenecer a la clase 1 y 1-P de pertenecer a la clase 2.\n",
    "\n",
    "**La idea es que la transición de la zona en la que los patrones pertenecen a la clase 1 y la zona en la que los patrones pertenecen a la clase 2, tenga una transición suave.**\n",
    "\n",
    "\n",
    "Una hipótesis para nuestro problema podría ser que un patrón en particular (x1,x2) tiene una probabilidad de ser :\n",
    "\n",
    "$$ P(h) = \\frac{1}{1 + e^{-h}} $$\n",
    "\n",
    "A la función definida anteriormente se la llama sigmoide y tiene la siguiente forma.\n",
    "\n",
    "<img src=\"../Imagenes/funcion_sigmoide.png\" width=50%>\n",
    "\n",
    "Cuando h=0, habrá una zona en la cual la probabilidad de que un patrón sea de la clase 1 es 0.5. Este valor definirá el umbral entre los patrones de tipo 1 y los patrones de tipo 2.  \n",
    "\n",
    "Si queremos que para nuestro problema de clasificación dicho umbral sea una recta, podemos definir:\n",
    "\n",
    "$$ h= \\beta_0+\\beta_1*x_1+\\beta_2*x_2 $$\n",
    "\n",
    "Nótese que cuando h=0, la probabilidad de un patrón de pertenecer a la clase 1, es 0.5. Esto ocurre en el espacio geométrico:\n",
    "\n",
    "$$ x_2=-\\frac{\\beta_1}{\\beta_2}.x_1-\\frac{\\beta_0}{\\beta_2} $$\n",
    "\n",
    "Estamos proponiendo un modelo probabilístico de parámetros $\\beta_1,\\beta_2,\\beta_0$ y por lo tanto la pregunta es:\n",
    "\n",
    "** ¿Cuáles son los valores de $\\beta_1,\\beta_2,\\beta_0$ que maximizan el likelihood de este modelo?**\n",
    "\n",
    "Para ello, podemos plantear el likelihood del modelo como la probabilidad de que este modelo haya generado los patrones observados. Si suponemos que las observaciones son independientes nos queda:\n",
    "\n",
    "$$\\mathcal{L} = \\prod_{i=1}^{N} Pr(Y=y_i|X=x_i) = \\prod_{i=1}^{N} p(x_i;\\beta_1,\\beta_2,\\beta_0)^{y_i}.[1 -p(x_i;\\beta_1,\\beta_2,\\beta_0)]^{1 - y_i} = $$\n",
    "$$=\\prod_{i=1}^{N} \\left(\\frac{1}{1 + e^{-(\\beta_0+\\beta_1*x_1+\\beta_2*x_2)}}\\right)^{y_i} . \\left(1- \\frac{1}{1 + e^{-(\\beta_0+\\beta_1*x_1+\\beta_2*x_2)}}\\right)^{1-y_i} $$\n",
    "\n",
    "Queremos encontrar los parámetros $\\beta_i$ que maximicen el likelihood. Como hemos visto anteriormente, maximizar el likelihood equivale a maximizar el logaritmo de éste, ya que el logaritmo es una función monótona creciente. Una forma de hacerlo es utilizar el metodo de Gradient Descent y minimizar el $-log(\\mathcal{L})$. Al $-log(\\mathcal{L})$ se lo denomina **cross-entropy** cost function y en general está definido como:\n",
    "\n",
    "$$-\\log(\\mathcal{L})=-\\sum_{i=1}^{N}\\left[y_i.\\log{p(x_i)}+(1-y_i)\\log{(1-p(x_i))}\\right]$$\n",
    "\n",
    "Para ello debemos calcular el valor de la derivada del $-log(\\mathcal{L})$. Se puede demostrar que:\n",
    "\n",
    "$$\\frac{\\partial{log(\\mathcal{L})}}{\\partial{\\beta_j}}=\\sum_{i=1}^{N}(y_i-P(x_i;\\beta_1,\\beta_2,\\beta_0)).x_{ij}=\\sum_{i=1}^{N}\\left(y_i-\\frac{1}{1 + e^{-(\\beta_0+\\beta_1*x_1+\\beta_2*x_2)}}\\right).x_{ij}$$\n",
    "\n",
    "Tal como en el caso del error cuadrático medio, se puede maximizar el likelihood evaluándolo para todos los datos (Batch GD), para algunos datos (MiniBatch GS) o para un solo dato (Stochastic GD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import randint\n",
    "np.set_printoptions(threshold=np.inf) #Print complete matrix/array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(feature_matrix, labels, feature_matrix_test):\n",
    "    lr = linear_model.LogisticRegression(tol=0.00000004)\n",
    "    \n",
    "    #Entrenamos el modelo\n",
    "    lr.fit(feature_matrix, labels)\n",
    "    \n",
    "    #Puntaje del modelo\n",
    "    score = lr.score(feature_matrix, labels)\n",
    "    \n",
    "    #Predice el resultado (La categoria resultante)\n",
    "    predict = lr.predict(feature_matrix_test)\n",
    "    \n",
    "    #Devuelve las probabilidades de cada una de las etiquetas\n",
    "    predict_proba = lr.predict_proba(feature_matrix_test)\n",
    "    \n",
    "    return score, predict, predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos clasificado los patrones de entrada en dos clases.\n",
    "¿Cómo se puede adaptar lo visto anteriormente en el caso de que $y_i$ pueda pertenecer a K clases?\n",
    "Para ello se define un estimador de probabilidad para cada una de las categorías.\n",
    "La probabilidad de que el patrón de entrada $x^i$ pertenezca a una categoría k es:\n",
    "\n",
    "$$P(Y=k|x_i,\\beta)=\\frac{e^{\\beta^k.x_i}}{\\sum_{j=1}^{K}{e^{\\beta^j.x_i}}}$$\n",
    "\n",
    "A esta función se la denomina función **softmax**. Esta ecuación proporciona una transición suave en cuanto a la probabilidad de pertenecer a cada clase.\n",
    "Nótese que para el caso de K=2, queda la función sigmoidea de parámetro $\\beta = \\beta^2-\\beta^1$\n",
    "\n",
    "El procedimiento para encontrar los parámetros $\\beta$ es el mismo que para la función logística. Se calcula el likelihood del modelo para un grupo de observaciones y se buscan los parámetros que maximizan dicho likelihood, es decir, minimizan la cross-entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticMultiRegression(feature_matrix, labels, feature_matrix_test):\n",
    "    lr = linear_model.LogisticRegression(multi_class='multinomial',solver='lbfgs',max_iter=100,verbose=5,C=0.01)\n",
    "    \n",
    "    #Entrenamos el modelo\n",
    "    lr.fit(feature_matrix, labels)\n",
    "    \n",
    "    #Puntaje del modelo\n",
    "    score = lr.score(feature_matrix, labels)\n",
    "    \n",
    "    #Predice el resultado (La categoria resultante)\n",
    "    predict = lr.predict(feature_matrix_test)\n",
    "    \n",
    "    #Devuelve las probabilidades de cada una de las etiquetas\n",
    "    predict_proba = lr.predict_proba(feature_matrix_test)\n",
    "    \n",
    "    return score, predict, predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPAM o HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Score 1.0\n",
      "Se predice que el mail es ['NN'] con un 91.5% de certeza\n",
      "\n",
      "\n",
      "Multiclass Logistic Regression\n",
      "Score 0.5714285714285714\n",
      "Se predice que el mail es ['HAM'] con un 44.5% de certeza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(['SPAM', 'HAM', 'HAM', 'SPAM', 'HAM', 'HAM', 'NN'])\n",
    "vocabulary = np.array(['alargue', 'automóvil', 'casa', 'novedoso'])\n",
    "# Simulo que lee los mails y obtiene esta cantidad de palabras\n",
    "train_mails = [\"alargue alargue alargue automóvil automóvil automóvil casa novedoso novedoso novedoso novedoso novedoso novedoso\", \n",
    "        \"alargue automóvil automóvil automóvil automóvil casa casa casa casa casa casa casa novedoso\",\n",
    "        \"alargue automóvil automóvil automóvil automóvil casa casa casa novedoso\",\n",
    "        \"alargue alargue alargue alargue automóvil automóvil casa novedoso novedoso novedoso novedoso novedoso\",\n",
    "        \"alargue automóvil automóvil automóvil casa casa casa casa novedoso novedoso\",\n",
    "        \"alargue alargue automóvil automóvil automóvil automóvil casa casa casa casa casa novedoso\",\n",
    "        \"alargue alargue alargue alargue alargue alargue alargue alargue alargue automóvil casa novedoso\"]\n",
    "test_mails = [\"alargue alargue alargue alargue alargue alargue alargue alargue alargue automóvil casa aguila\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(train_mails)\n",
    "test_matrix = vectorizer.transform(test_mails)\n",
    "\n",
    "score, predict, predict_proba = LogisticRegression(train_matrix, labels, test_matrix)\n",
    "percent = round(float(np.amax(predict_proba, axis=1)) * 100, 2)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Score\", score)\n",
    "print(f\"Se predice que el mail es {predict} con un {percent}% de certeza\")\n",
    "print(\"\\n\")\n",
    "\n",
    "score, predict, predict_proba = LogisticMultiRegression(train_matrix, labels, test_matrix)\n",
    "percent = round(float(np.amax(predict_proba, axis=1)) * 100, 2)\n",
    "print(\"Multiclass Logistic Regression\")\n",
    "print(\"Score\", score)\n",
    "print(f\"Se predice que el mail es {predict} con un {percent}% de certeza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9692532942898975\n",
      "Se predice que la muestra es del tipo maligno con un 99.98% de certeza\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(fname='../datasets/cancer.txt', delimiter=',')\n",
    "data = np.delete(data, np.s_[0], axis=1)\n",
    "data = data[~np.isnan(data).any(axis=1)] #Eliminamos los nan\n",
    "\n",
    "labels = data[:, -1]\n",
    "features = data[:, :-1]\n",
    "\n",
    "test = np.matrix([10,5,6,10,6,10,7,7,10])\n",
    "score, predict, predict_proba = LogisticRegression(features, labels, test)\n",
    "\n",
    "cancer_type = \"benigno\" if int(predict) == 2 else \"maligno\"\n",
    "percent = round(float(np.amax(predict_proba, axis=1)) * 100, 2)\n",
    "print(\"Score: \", score)\n",
    "print(f\"Se predice que la muestra es del tipo {cancer_type} con un {percent}% de certeza\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
